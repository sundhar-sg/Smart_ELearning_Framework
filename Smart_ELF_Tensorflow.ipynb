{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smart E - Learning Framework using Deep Q - Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 20:13:24.655744: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-26 20:13:25.231839: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-26 20:13:25.232173: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-26 20:13:25.331533: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-26 20:13:25.522440: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-26 20:13:27.647576: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking whether Tensorflow able to detect the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 20:13:51.784317: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-26 20:13:52.306502: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-26 20:13:52.306644: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.list_physical_devices(device_type='GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States defined for the environment\n",
      "\n",
      "State 0 - Reflects Start\n",
      "State 1 - Reflects Reading\n",
      "State 2 - Reflects Watching(VL)\n",
      "State 3 - Reflects Entertaining\n",
      "State 4 - Reflects GettingBoredOrFrustration\n",
      "State 5 - Reflects Rest/Sleep\n",
      "State 6 - Reflects Writing\n",
      "State 7 - Reflects PlayingaGame\n",
      "State 8 - Reflects ClickingOnAnAd\n",
      "State 9 - Reflects CourseCompletion\n",
      "State 10 - Reflects QuitStudyOrDisengagement\n"
     ]
    }
   ],
   "source": [
    "states = {}\n",
    "\n",
    "states[0] = \"Start\"\n",
    "states[1] = \"Reading\"\n",
    "states[2] = \"Watching(VL)\"\n",
    "states[3] = \"Entertaining\"\n",
    "states[4] = \"GettingBoredOrFrustration\"\n",
    "states[5] = \"Rest/Sleep\"\n",
    "states[6] = \"Writing\"\n",
    "states[7] = \"PlayingaGame\"\n",
    "states[8] = \"ClickingOnAnAd\"\n",
    "states[9] = \"CourseCompletion\"\n",
    "states[10] = \"QuitStudyOrDisengagement\"\n",
    "\n",
    "print(\"States defined for the environment\\n\")\n",
    "for key in states.keys():\n",
    "    print(\"State {} - Reflects {}\".format(key, states[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions defined for the environment\n",
      "\n",
      "Action 1 - Reflects Stay\n",
      "Action 2 - Reflects PreviousCourse\n",
      "Action 3 - Reflects AdditionalContent\n",
      "Action 4 - Reflects DoAssignment/Quiz\n",
      "Action 5 - Reflects MoveToExams\n",
      "Action 6 - Reflects MoveToHighLevelCourse\n",
      "Action 7 - Reflects MoveToLowLevelCourse\n",
      "Action 8 - Reflects MoveToSocialMedia\n"
     ]
    }
   ],
   "source": [
    "actions = {}\n",
    "\n",
    "actions[1] = \"Stay\"\n",
    "actions[2] = \"PreviousCourse\"\n",
    "actions[3] = \"AdditionalContent\"\n",
    "actions[4] = \"DoAssignment/Quiz\"\n",
    "actions[5] = \"MoveToExams\"\n",
    "actions[6] = \"MoveToHighLevelCourse\"\n",
    "actions[7] = \"MoveToLowLevelCourse\"\n",
    "actions[8] = \"MoveToSocialMedia\"\n",
    "\n",
    "print(\"Actions defined for the environment\\n\")\n",
    "for key in actions.keys():\n",
    "    print(\"Action {} - Reflects {}\".format(key, actions[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Rewards for each action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards assigned for each of the action\n",
      "\n",
      "Action: Stay - Reward: 10\n",
      "Action: PreviousCourse - Reward: 20\n",
      "Action: AdditionalContent - Reward: 50\n",
      "Action: DoAssignment/Quiz - Reward: 60\n",
      "Action: MoveToExams - Reward: 100\n",
      "Action: MoveToHighLevelCourse - Reward: 100\n",
      "Action: MoveToLowLevelCourse - Reward: 70\n",
      "Action: MoveToSocialMedia - Reward: -10\n"
     ]
    }
   ],
   "source": [
    "rewards = {}\n",
    "\n",
    "initialRewards = [10, 20, 50, 60, 100, 100, 70, -10]\n",
    "\n",
    "for key in actions.keys():\n",
    "    rewards[actions[key]] = initialRewards[key - 1]\n",
    "\n",
    "print(\"Rewards assigned for each of the action\\n\")\n",
    "for key in rewards.keys():\n",
    "    print(\"Action: {} - Reward: {}\".format(key, rewards[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the custom environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State size:  11\n",
      "Number of Actions:  8\n"
     ]
    }
   ],
   "source": [
    "class Environment(gym.Env):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Environment, self).__init__()\n",
    "        self.num_states = len(states)\n",
    "        self.num_actions = len(actions)\n",
    "        self.action_space = spaces.Discrete(self.num_actions)\n",
    "        self.observation_space = spaces.Discrete(self.num_states)\n",
    "        self.current_state = None\n",
    "\n",
    "        self.rewards = {\n",
    "            0: 10,\n",
    "            1: 20,\n",
    "            2: 50,\n",
    "            3: 60,\n",
    "            4: 100,\n",
    "            5: 100,\n",
    "            6: 70,\n",
    "            7: -10\n",
    "        }\n",
    "\n",
    "    def reset(self) :\n",
    "        self.current_state = np.random.randint(0, self.num_states)\n",
    "        return self.current_state\n",
    "    \n",
    "    def step(self, action) :\n",
    "        # Take action and transition to the next state based on the action\n",
    "        if action in range(self.num_actions) :\n",
    "            self.current_state = (self.current_state + action) % self.num_states\n",
    "        else :\n",
    "            raise ValueError(\"Invalid action\")\n",
    "        \n",
    "        # Get reward based on the action taken\n",
    "        reward = self.rewards.get(action, 0)\n",
    "\n",
    "        done = False\n",
    "\n",
    "        return self.current_state, reward, done, {}\n",
    "    \n",
    "    def render(self, mode='human') :\n",
    "        print(f\"Current State: {self.current_state}\")\n",
    "\n",
    "gym.register(id=\"Custom_ELearning_Env-v1\", entry_point=\"elearningEnv:Custom_ELearning_Env\")\n",
    "# Defining the Environment class\n",
    "environment = Environment()\n",
    "\n",
    "# Calculate the State size and number of actions\n",
    "stateSize = environment.observation_space.n\n",
    "numberActions = environment.action_space.n\n",
    "\n",
    "print(\"State size: \", stateSize)\n",
    "print(\"Number of Actions: \", numberActions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNetwork(tf.keras.Model):\n",
    "    def __init__(self, state_size, action_size, seed = 42) :\n",
    "        super(NNetwork, self).__init__()\n",
    "        self.seed = tf.random.set_seed(seed)\n",
    "        self.fc1 = tf.keras.layers.Dense(64, activation=\"relu\")\n",
    "        self.fc2 = tf.keras.layers.Dense(128, activation=\"relu\")\n",
    "        self.fc3 = tf.keras.layers.Dense(128, activation=\"relu\")\n",
    "        self.fc4 = tf.keras.layers.Dense(64, activation=\"relu\")\n",
    "        self.fc5 = tf.keras.layers.Dense(action_size)\n",
    "\n",
    "\n",
    "    def call(self, state):\n",
    "        x = self.fc1(state)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.fc5(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "learningRate = 1e-4\n",
    "miniBatchSize = 100\n",
    "discountFactor = 0.99\n",
    "replayBufferSize = int(1e5)\n",
    "interpolationParameter = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.device = tf.device(\"/GPU:0\" if tf.test.is_gpu_available() else \"/CPU:0\")\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "\n",
    "    def push(self, event):\n",
    "        self.memory.append(event)\n",
    "        if len(self.memory) > self.capacity:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        experiences = random.sample(self.memory, k = batch_size) \n",
    "        states = tf.convert_to_tensor(np.vstack([e[0] for e in experiences if e is not None]), dtype=tf.float32)\n",
    "        actions = tf.convert_to_tensor(np.vstack([e[1] for e in experiences if e is not None]), dtype=tf.int64)\n",
    "        rewards = tf.convert_to_tensor(np.vstack([e[2] for e in experiences if e is not None]), dtype=tf.float32)\n",
    "        next_states = tf.convert_to_tensor(np.vstack([e[3] for e in experiences if e is not None]), dtype=tf.float32)\n",
    "        dones = tf.convert_to_tensor(np.vstack([e[4] for e in experiences if e is not None]).astype(np.uint8), dtype=tf.float32)\n",
    "        return states, actions, rewards, next_states, dones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the Deep Q - Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.device = tf.device(device_name=\"/GPU:0\" if tf.config.list_physical_devices('GPU') else \"/CPU:0\")\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.local_network = NNetwork(state_size, action_size)\n",
    "        self.target_network = NNetwork(state_size, action_size)\n",
    "        self.optimizer = tf.optimizers.Adam(learning_rate=learningRate)\n",
    "        self.memory = ReplayMemory(replayBufferSize)\n",
    "        self.t_step = 0\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        self.memory.push((state, action, reward, next_state, done))\n",
    "        self.t_step = (self.t_step + 1) % 4\n",
    "        if self.t_step == 0:\n",
    "            if len(self.memory.memory) > miniBatchSize:\n",
    "                experiences = self.memory.sample(100)\n",
    "                self.learn(experiences, discountFactor)\n",
    "\n",
    "    def act(self, state, epsilon = 0.):\n",
    "        state = tf.expand_dims(tf.convert_to_tensor(state, dtype=tf.float32), axis = 0)\n",
    "        self.local_network.eval()\n",
    "        action_values = self.local_network(state)\n",
    "        self.local_network.train()\n",
    "        if random.random() > epsilon:\n",
    "            return tf.math.argmax(action_values.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "        \n",
    "    def learn(self, experiences, discount_factor):\n",
    "        states, next_states, actions, rewards, dones = experiences\n",
    "        next_q_targets = self.target_network(next_states).numpy().max(axis=1).reshape((-1, 1))\n",
    "        q_targets = rewards + discount_factor * next_q_targets * (1 - dones)\n",
    "        with tf.GradientTape() as tape:\n",
    "            q_expected = self.local_network(states)\n",
    "            actions_one_hot = tf.one_hot(actions, self.action_size)\n",
    "            q_expected_action = tf.reduce_sum(q_expected * actions_one_hot, axis=1, keepdims=True)\n",
    "            loss = tf.reduce_mean(tf.square(q_expected_action - q_targets))\n",
    "        gradients = tape.gradient(loss, self.local_network.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.local_network.trainable_variables))\n",
    "        self.soft_update(self.local_network, self.target_network, interpolationParameter)\n",
    "\n",
    "    def soft_update(self, local_model, target_model, interpolation_parameter) :\n",
    "        for target_param, local_param in zip(target_model.trainable_variables, local_model.trainable_parameters) :\n",
    "            target_param.assign(interpolation_parameter * local_param + (1.0 - interpolation_parameter) * target_param)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the Deep Q - Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_159450/2271861769.py:4: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 20:15:20.200340: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-26 20:15:20.200460: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-26 20:15:20.200479: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-26 20:15:20.365222: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-26 20:15:20.365361: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-26 20:15:20.365372: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-02-26 20:15:20.365417: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-26 20:15:20.365475: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1753 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Ti Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2024-02-26 20:15:20.471679: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-26 20:15:20.471756: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-26 20:15:20.471774: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-26 20:15:20.472151: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-26 20:15:20.472166: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-02-26 20:15:20.472198: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-26 20:15:20.472209: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /device:GPU:0 with 1753 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Ti Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(stateSize, numberActions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Deep Q - Network Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NNetwork' object has no attribute 'eval'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(maximumNumberTimestampsPerEpisode) :\n\u001b[0;32m---> 13\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     next_state, reward, done, _, _ \u001b[38;5;241m=\u001b[39m environment\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     15\u001b[0m     agent\u001b[38;5;241m.\u001b[39mstep(state, action, reward, next_state, done)\n",
      "Cell \u001b[0;32mIn[13], line 23\u001b[0m, in \u001b[0;36mAgent.act\u001b[0;34m(self, state, epsilon)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mact\u001b[39m(\u001b[38;5;28mself\u001b[39m, state, epsilon \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.\u001b[39m):\n\u001b[1;32m     22\u001b[0m     state \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mexpand_dims(tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(state, dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32), axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_network\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m()\n\u001b[1;32m     24\u001b[0m     action_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_network(state)\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_network\u001b[38;5;241m.\u001b[39mtrain()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NNetwork' object has no attribute 'eval'"
     ]
    }
   ],
   "source": [
    "numberEpisodes = 100\n",
    "maximumNumberTimestampsPerEpisode = 1000\n",
    "epsilonStartingValue = 1.0\n",
    "epsilonEndingValue = 0.01\n",
    "epsilonDecayValue = 0.995\n",
    "epsilon = epsilonStartingValue\n",
    "scoresOn10Episodes = deque(maxlen = 10)\n",
    "\n",
    "for episode in range(1, numberEpisodes + 1) :\n",
    "    state = environment.reset()\n",
    "    score = 0\n",
    "    for t in range(maximumNumberTimestampsPerEpisode) :\n",
    "        action = agent.act(state, epsilon)\n",
    "        next_state, reward, done, _, _ = environment.step(action)\n",
    "        agent.step(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        score += reward\n",
    "        if done :\n",
    "            break\n",
    "    scoresOn10Episodes.append(score)\n",
    "    epsilon = max(epsilonEndingValue, epsilonDecayValue * epsilon)\n",
    "    print('\\rEpisode: {}\\tAverage Score: {:.2f}'.format(episode, np.mean(scoresOn10Episodes)), end='')\n",
    "    if episode % 10 == 0 :\n",
    "        print(\"\\rEpisode: {}\\tAverage Score: {:.2f}\".format(episode, np.mean(scoresOn10Episodes)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
