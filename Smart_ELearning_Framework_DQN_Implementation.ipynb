{"cells":[{"cell_type":"markdown","metadata":{"id":"Vi1nhs_6_NnV"},"source":["## Smart E - Learning Framework Using Deep Q - Network of Reinforcement Learning"]},{"cell_type":"markdown","metadata":{"id":"kQn4bzRR_y5S"},"source":["### Installing the Necessary Libraries"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":109420,"status":"ok","timestamp":1709106373938,"user":{"displayName":"Sundhar Gopal","userId":"16938864680908115120"},"user_tz":-330},"id":"wFImd1ao_2jn","outputId":"0b2c18b4-9568-432a-9904-db54d7283f93"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: gymnasium in /home/sundhar-sg/anaconda3/envs/tensorflow/lib/python3.11/site-packages (0.28.1)\n","Requirement already satisfied: numpy>=1.21.0 in /home/sundhar-sg/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from gymnasium) (1.23.5)\n","Requirement already satisfied: jax-jumpy>=1.0.0 in /home/sundhar-sg/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from gymnasium) (1.0.0)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /home/sundhar-sg/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from gymnasium) (2.2.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /home/sundhar-sg/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from gymnasium) (4.9.0)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /home/sundhar-sg/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from gymnasium) (0.0.4)\n","Requirement already satisfied: gymnasium[accept-rom-license,atari] in /home/sundhar-sg/anaconda3/envs/tensorflow/lib/python3.11/site-packages (0.28.1)\n","Requirement already satisfied: numpy>=1.21.0 in /home/sundhar-sg/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from gymnasium[accept-rom-license,atari]) (1.23.5)\n","Requirement already satisfied: jax-jumpy>=1.0.0 in /home/sundhar-sg/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from gymnasium[accept-rom-license,atari]) (1.0.0)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /home/sundhar-sg/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from gymnasium[accept-rom-license,atari]) (2.2.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /home/sundhar-sg/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from gymnasium[accept-rom-license,atari]) (4.9.0)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /home/sundhar-sg/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from gymnasium[accept-rom-license,atari]) (0.0.4)\n","Requirement already satisfied: autorom~=0.4.2 in /home/sundhar-sg/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari]) (0.4.2)\n","Requirement already satisfied: shimmy<1.0,>=0.1.0 in /home/sundhar-sg/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[accept-rom-license,atari]) (0.2.1)\n","Requirement already satisfied: click in /home/sundhar-sg/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari]) (8.1.7)\n","Requirement already satisfied: requests in /home/sundhar-sg/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari]) (2.31.0)\n","Requirement already satisfied: tqdm in /home/sundhar-sg/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari]) (4.66.2)\n","Requirement already satisfied: AutoROM.accept-rom-license in /home/sundhar-sg/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari]) (0.6.1)\n","Requirement already satisfied: ale-py~=0.8.1 in /home/sundhar-sg/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[accept-rom-license,atari]) (0.8.1)\n","Requirement already satisfied: importlib-resources in /home/sundhar-sg/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[accept-rom-license,atari]) (6.1.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /home/sundhar-sg/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari]) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /home/sundhar-sg/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari]) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /home/sundhar-sg/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari]) (2.2.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /home/sundhar-sg/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari]) (2024.2.2)\n","E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)\n","E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?\n","Requirement already satisfied: gymnasium[box2d] in /home/sundhar-sg/anaconda3/envs/tensorflow/lib/python3.11/site-packages (0.28.1)\n","Requirement already satisfied: numpy>=1.21.0 in /home/sundhar-sg/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from gymnasium[box2d]) (1.23.5)\n","Requirement already satisfied: jax-jumpy>=1.0.0 in /home/sundhar-sg/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from gymnasium[box2d]) (1.0.0)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /home/sundhar-sg/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from gymnasium[box2d]) (2.2.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /home/sundhar-sg/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from gymnasium[box2d]) (4.9.0)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /home/sundhar-sg/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from gymnasium[box2d]) (0.0.4)\n","Requirement already satisfied: box2d-py==2.3.5 in /home/sundhar-sg/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from gymnasium[box2d]) (2.3.5)\n","Requirement already satisfied: pygame==2.1.3 in /home/sundhar-sg/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from gymnasium[box2d]) (2.1.3)\n","Requirement already satisfied: swig==4.* in /home/sundhar-sg/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from gymnasium[box2d]) (4.2.1)\n"]}],"source":["!pip install gymnasium\n","!pip install \"gymnasium[atari, accept-rom-license]\"\n","!apt-get install swig\n","!pip install gymnasium[box2d]"]},{"cell_type":"markdown","metadata":{"id":"CUDsmBeX_ct9"},"source":["### Importing the Libraries"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":5006,"status":"ok","timestamp":1709106378937,"user":{"displayName":"Sundhar Gopal","userId":"16938864680908115120"},"user_tz":-330},"id":"tPTc-3H__e25"},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-02-28 21:51:15.202518: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2024-02-28 21:51:15.333318: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-02-28 21:51:15.333432: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-02-28 21:51:15.346911: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-02-28 21:51:15.401551: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-02-28 21:51:16.932155: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"]}],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","\n","import tensorflow as tf\n","import gymnasium as gym\n","from gymnasium import spaces\n","\n","from collections import deque, namedtuple\n","\n","from tensorflow import keras\n","from keras import Sequential\n","from keras.layers import Dense, Input\n","from keras.losses import MSE\n","from keras.optimizers import Adam"]},{"cell_type":"markdown","metadata":{"id":"7H7e-uVm2Fpd"},"source":["### Defining the States for the Environment"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1709106378937,"user":{"displayName":"Sundhar Gopal","userId":"16938864680908115120"},"user_tz":-330},"id":"_brb4IZl2JGa","outputId":"cc90de95-fced-47ad-eafc-5eefc00860fa"},"outputs":[{"name":"stdout","output_type":"stream","text":["State 1: Start\n","State 2: Reading\n","State 3: Watching\n","State 4: Entertaining\n","State 5: Getting Bored\n","State 6: Rest\n","State 7: Writing\n","State 8: Playing\n","State 9: Clicking Ad\n","State 10: Course Completion\n","State 11: Quit\n"]}],"source":["states = ['Start', 'Reading', 'Watching', 'Entertaining', 'Getting Bored', 'Rest', 'Writing', 'Playing', 'Clicking Ad', 'Course Completion', 'Quit']\n","\n","for i in range(len(states)) :\n","    print(\"State {}: {}\".format(i+1, states[i]))"]},{"cell_type":"markdown","metadata":{"id":"gnhNxfoe2y5G"},"source":["### Defining the Actions for the Environment"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1709106378938,"user":{"displayName":"Sundhar Gopal","userId":"16938864680908115120"},"user_tz":-330},"id":"TcjC9HL023MA","outputId":"2ce1da02-00bb-41ab-99e4-926cf0975b25"},"outputs":[{"name":"stdout","output_type":"stream","text":["Action 1: Stay\n","Action 2: Previous Course\n","Action 3: Additional Content\n","Action 4: Do assignment\n","Action 5: Move to Exams\n","Action 6: Move to high-level course\n","Action 7: Move to low-level course\n","Action 8: Move to Social Media\n"]}],"source":["actions = ['Stay', 'Previous Course', 'Additional Content', 'Do assignment', 'Move to Exams', 'Move to high-level course', 'Move to low-level course', 'Move to Social Media']\n","\n","for i in range(len(actions)) :\n","    print(\"Action {}: {}\".format(i+1, actions[i]))"]},{"cell_type":"markdown","metadata":{"id":"OyzJpoWK4oH8"},"source":["### Assigning Rewards for each action to be taken respectively"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1709106378938,"user":{"displayName":"Sundhar Gopal","userId":"16938864680908115120"},"user_tz":-330},"id":"t_wntOap4yT7","outputId":"cd365ea9-6482-4f08-c74d-11d86cbc3555"},"outputs":[{"name":"stdout","output_type":"stream","text":["Action: Stay\tReward Points: 10\n","Action: Previous Course\tReward Points: 20\n","Action: Additional Content\tReward Points: 50\n","Action: Do assignment\tReward Points: 60\n","Action: Move to Exams\tReward Points: 100\n","Action: Move to high-level course\tReward Points: 100\n","Action: Move to low-level course\tReward Points: 70\n","Action: Move to Social Media\tReward Points: -10\n"]}],"source":["rewards = {}\n","rewardPoints = [10, 20, 50, 60, 100, 100, 70, -10]\n","\n","for i in range(len(actions)) :\n","    rewards['{}'.format(actions[i])] = rewardPoints[i]\n","\n","keys = list(rewards.keys())\n","for i in range(len(rewards)) :\n","    print(\"Action: {}\\tReward Points: {}\".format(keys[i], rewards.get(keys[i])))"]},{"cell_type":"markdown","metadata":{"id":"pAHepFAKBFYN"},"source":["### Creating and Registering the Environment in Gymnasium"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1709106378939,"user":{"displayName":"Sundhar Gopal","userId":"16938864680908115120"},"user_tz":-330},"id":"7TY2KDw_BLAy"},"outputs":[],"source":["class ELearningEnv(gym.Env) :\n","    def __init__(self) :\n","        super(ELearningEnv, self).__init__()\n","        self.states = states\n","        self.actions = actions\n","        self.rewards = rewards\n","        self.action_space = spaces.discrete.Discrete(len(self.actions))\n","        self.observation_space = spaces.discrete.Discrete(len(self.states))\n","        self.current_state = None\n","\n","    def reset(self) :\n","        self.current_state = 0\n","        return self.current_state\n","\n","    def step(self, action) :\n","        reward = self.rewards.get(self.actions[action], 0)\n","        done = (self.states[self.current_state] in ['Course Completion', 'Quit'])\n","        if not done :\n","            if action == 0 :\n","                pass\n","            elif action == 1 :\n","                self.current_state = max(0, self.current_state - 1)\n","            elif action == 2 :\n","                self.current_state = min(len(self.states) - 1, self.current_state + 1)\n","            elif action == 3 :\n","                self.current_state = min(len(self.states) - 2, self.current_state + 2)\n","            elif action == 4 :\n","                self.current_state = min(len(self.states) - 3, self.current_state + 3)\n","            elif action == 5 :\n","                self.current_state = min(len(self.states) - 4, self.current_state + 4)\n","            elif action == 6 :\n","                self.current_state = min(len(self.states) - 5, self.current_state + 5)\n","            elif action == 7 :\n","                self.current_state = min(len(self.states) - 6, self.current_state + 6)\n","        return self.current_state, reward, done, {}\n","\n","    def render(self, mode = \"human\") :\n","        print(f\"Current State: {self.states[self.current_state]}\")\n","\n","gym.register(id=\"ELearningEnv-v1\", entry_point=\"__main__:ELearningEnv\")"]},{"cell_type":"markdown","metadata":{"id":"Cb8JzWgL86JJ"},"source":["### Setting the Seeding value for the Environment"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1709106378939,"user":{"displayName":"Sundhar Gopal","userId":"16938864680908115120"},"user_tz":-330},"id":"l0xJGZvo89_Y"},"outputs":[],"source":["tf.random.set_seed(0)"]},{"cell_type":"markdown","metadata":{"id":"iEesj09B9Oa_"},"source":["### Defining the Hyperparameters"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":404,"status":"ok","timestamp":1709106379335,"user":{"displayName":"Sundhar Gopal","userId":"16938864680908115120"},"user_tz":-330},"id":"_y_mF1rj9RQw"},"outputs":[],"source":["MEMORY_SIZE = 100_000\n","GAMMA = 0.995\n","ALPHA = 1e-3\n","NUM_STEPS_FOR_EPISODE = 4"]},{"cell_type":"markdown","metadata":{"id":"rmnSlEEf9i4x"},"source":["### Defining the Environment for use"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1709106379336,"user":{"displayName":"Sundhar Gopal","userId":"16938864680908115120"},"user_tz":-330},"id":"HcvOsiZ49u5v","outputId":"1460c902-55ef-42d1-ebbf-3433c15335ef"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of States Available:  11 \n","Number of Actions:  8\n"]},{"name":"stderr","output_type":"stream","text":["/home/sundhar-sg/anaconda3/envs/tensorflow/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:197: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n","  logger.deprecation(\n","/home/sundhar-sg/anaconda3/envs/tensorflow/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:210: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n","  logger.deprecation(\n","/home/sundhar-sg/anaconda3/envs/tensorflow/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:218: UserWarning: \u001b[33mWARN: The result returned by `env.reset()` was not a tuple of the form `(obs, info)`, where `obs` is a observation and `info` is a dictionary containing additional information. Actual type: `<class 'int'>`\u001b[0m\n","  logger.warn(\n"]}],"source":["env = gym.make(\"ELearningEnv-v1\")\n","env.reset()\n","\n","state_size = env.observation_space.n\n","action_size = env.action_space.n\n","\n","print(\"Number of States Available: \", state_size, \"\\nNumber of Actions: \", action_size)"]},{"cell_type":"markdown","metadata":{"id":"q1MFnkNaQaZh"},"source":["### Reset the Environment"]},{"cell_type":"code","execution_count":28,"metadata":{"executionInfo":{"elapsed":19,"status":"ok","timestamp":1709106379336,"user":{"displayName":"Sundhar Gopal","userId":"16938864680908115120"},"user_tz":-330},"id":"hlFTIL8OQfGF"},"outputs":[],"source":["current_state = env.reset()"]},{"cell_type":"markdown","metadata":{"id":"jZUI5csGQu71"},"source":["### Test the Environment for a action"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1709106379336,"user":{"displayName":"Sundhar Gopal","userId":"16938864680908115120"},"user_tz":-330},"id":"FADOf4DJQybA","outputId":"43db24f0-77f9-4fe3-c61f-b1b0ca96c67a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Current State: Getting Bored\n"]}],"source":["action = 5\n","\n","next_state, reward, done, _ = env.step(action)\n","\n","env.render()"]},{"cell_type":"markdown","metadata":{"id":"q_Vh7aXWU9_R"},"source":["### Create the Deep Neural Network for the Q - Network"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1709106379337,"user":{"displayName":"Sundhar Gopal","userId":"16938864680908115120"},"user_tz":-330},"id":"pYydVBrZVDku"},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-02-28 21:51:20.064080: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n","Your kernel may have been built without NUMA support.\n","2024-02-28 21:51:20.177366: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n","Your kernel may have been built without NUMA support.\n","2024-02-28 21:51:20.177439: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n","Your kernel may have been built without NUMA support.\n","2024-02-28 21:51:20.183734: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n","Your kernel may have been built without NUMA support.\n","2024-02-28 21:51:20.183814: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n","Your kernel may have been built without NUMA support.\n","2024-02-28 21:51:20.183854: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n","Your kernel may have been built without NUMA support.\n","2024-02-28 21:51:20.449155: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n","Your kernel may have been built without NUMA support.\n","2024-02-28 21:51:20.449311: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n","Your kernel may have been built without NUMA support.\n","2024-02-28 21:51:20.449327: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n","2024-02-28 21:51:20.449403: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n","Your kernel may have been built without NUMA support.\n","2024-02-28 21:51:20.449494: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1753 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Ti Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"]}],"source":["QNetwork = Sequential(\n","    [\n","        Input(shape=(state_size,)),\n","        Dense(units=64, activation=\"relu\"),\n","        Dense(units=128, activation=\"relu\"),\n","        Dense(units=64, activation=\"relu\"),\n","        Dense(units=action_size, activation=\"linear\")\n","    ]\n",")\n","\n","target_QNetwork = Sequential(\n","    [\n","        Input(shape=(state_size,)),\n","        Dense(units=64, activation=\"relu\"),\n","        Dense(units=128, activation=\"relu\"),\n","        Dense(units=64, activation=\"relu\"),\n","        Dense(units=action_size, activation=\"linear\")\n","    ]\n",")\n","\n","optimizer = Adam(learning_rate=ALPHA)"]},{"cell_type":"markdown","metadata":{"id":"lczeVADlfJjy"},"source":["### Implementing Experience Replay"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1709106379337,"user":{"displayName":"Sundhar Gopal","userId":"16938864680908115120"},"user_tz":-330},"id":"XURocGktfQ-o"},"outputs":[],"source":["experience = namedtuple(typename=\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])"]},{"cell_type":"markdown","metadata":{"id":"yCGQaH5lgQk0"},"source":["### Defining Compute Loss function for the Deep Q - Learning with Experience Replay"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":401,"status":"ok","timestamp":1709108553326,"user":{"displayName":"Sundhar Gopal","userId":"16938864680908115120"},"user_tz":-330},"id":"OnAeyxWwgXIx"},"outputs":[],"source":["def compute_loss(experiences, gamma, QNetwork, target_QNetwork) :\n","\n","    # Unpack the mini-batch of experiences\n","    states, actions, rewards, next_states, done_vals = experiences\n","\n","    # Compute max Q^(s,a)\n","    max_qsa = tf.reduce_max(input_tensor=target_QNetwork(next_states), axis=-1)\n","\n","    # Set y = R if episode terminates, otherwise set y = R + gamma max Q^(s,a)\n","    yTargets = rewards + (gamma * max_qsa * (1 - done_vals))\n","\n","    # Get the q_values and reshape to match y_targets\n","    q_values = QNetwork(states)\n","    q_values = tf.gather_nd(q_values, tf.stack([tf.range(q_values.shape[0]),\n","                                                tf.cast(actions, tf.int32)], axis = 1))\n","    \n","    # Compute the Loss\n","    loss = MSE(yTargets, q_values)\n","\n","    return loss"]},{"cell_type":"markdown","metadata":{},"source":["### Update the Target Network Weights"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["TAU = 1e-3 # Soft update parameter\n","\n","def update_target_network(QNetwork, target_QNetwork) :\n","    for targetWeights, QNetWeights in zip(target_QNetwork.weights, QNetwork.weights) :\n","        targetWeights.assign(TAU * QNetWeights + (1.0 - TAU) * targetWeights)"]},{"cell_type":"markdown","metadata":{},"source":["### Getting the Action based on Epsilon-greedy policy"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import random\n","\n","def get_action(q_values, epsilon=0.0) :\n","\n","    # Returns an action based on the Epsilon-greedy policy\n","\n","    if random.random() > epsilon :\n","        return np.argmax(q_values.numpy()[0])\n","    return random.choice(np.arange(8))"]},{"cell_type":"markdown","metadata":{},"source":["### Checking the Update conditions for the network"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["MINIBATCH_SIZE = 64\n","\n","def check_update_conditions(t, num_steps_upd, memory_buffer) :\n","\n","    # Returns a boolean that will be True if the conditions are met and False otherwise.\n","\n","    if (t + 1) % num_steps_upd == 0 and len(memory_buffer) > MINIBATCH_SIZE :\n","        return True\n","    return False"]},{"cell_type":"markdown","metadata":{},"source":["### Getting the Experiences Randomly from the Memory Buffer"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_experiences(memory_buffer) :\n","\n","    # Returns a random sample of experience tuples drawn from the memory buffer.\n","    experiences = random.sample(memory_buffer, k=MINIBATCH_SIZE)\n","    states = tf.convert_to_tensor(\n","        np.array([e.state for e in experiences if e is not None]), dtype=tf.float32\n","    )\n","    actions = tf.convert_to_tensor(\n","        np.array([e.action for e in experiences if e is not None]), dtype=tf.float32\n","    )\n","    rewards = tf.convert_to_tensor(\n","        np.array([e.reward for e in experiences if e is not None]), dtype=tf.float32\n","    )\n","    next_states = tf.convert_to_tensor(\n","        np.array([e.next_state for e in experiences if e is not None]), dtype=tf.float32\n","    )\n","    done_vals = tf.convert_to_tensor(\n","        np.array([e.done for e in experiences if e is not None]).astype(np.uint8),\n","        dtype=tf.float32,\n","    )\n","    return (states, actions, rewards, next_states, done_vals)"]},{"cell_type":"markdown","metadata":{},"source":["### Updating the Epsilon value"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["EPSILON_MIN = 0.01 # Minimum Epsilon value for the Epsilon-greedy policy\n","EPSILON_DECAY = 0.995 # Epsilon-decay rate for the Epsilon-greedy policy\n","\n","def get_new_epsilon(epsilon) :\n","\n","    # Returns the updated Epsilon value for the Epsilon-greedy policy\n","\n","    return max(EPSILON_MIN, EPSILON_DECAY * epsilon)"]},{"cell_type":"markdown","metadata":{},"source":["### Update the Network Weights"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["@tf.function\n","def agent_learn(experiences, gamma) :\n","    \n","    # Calculate the loss\n","    with tf.GradientTape() as tape :\n","        loss = compute_loss(experiences, gamma, QNetwork, target_QNetwork)\n","\n","    # Get the Gradients of the loss with respect to the weights\n","    gradients = tape.gradient(loss, QNetwork.trainable_variables)\n","\n","    # Update the weights of the QNetwork\n","    optimizer.apply_gradients(zip(gradients, QNetwork.trainable_variables))\n","\n","    # Update the Weights of the Target QNetwork\n","    update_target_network(QNetwork, target_QNetwork)"]},{"cell_type":"markdown","metadata":{},"source":["### Training the Agent"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'num' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[20], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m maximumNumberTimesteps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m\n\u001b[1;32m      9\u001b[0m totalPointHistory \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 11\u001b[0m \u001b[43mnum\u001b[49m\n","\u001b[0;31mNameError\u001b[0m: name 'num' is not defined"]}],"source":["import time\n","\n","\n","start = time.time()\n","\n","numberOfEpisodes = 100\n","maximumNumberTimesteps = 500\n","\n","totalPointHistory = []\n","\n","numberPointAverage = 10 # Number of total points to use for averaging\n","epsilon = 1.0 # Initial Epsilon value for Epsilon-greedy policy\n","\n","# Create a memory buffer D with capacity N\n","memory_buffer = deque(maxlen=MEMORY_SIZE)\n","\n","# Set the target network weights equal to the Q-Network weights\n","target_QNetwork.set_weights(QNetwork.get_weights())\n","\n","for i in range(numberOfEpisodes) :\n","\n","    # Reset the environment to the initial state and get the initial state\n","    state = env.reset()\n","    totalPoints = 0\n","\n","    for t in range(maximumNumberTimesteps) :\n","\n","        # From the current state S, choose an action A using an Epsilon-greedy policy\n","        state_qn = np.expand_dims(state, axis=0) # State needs to be the right shape for the Q-Network\n","        q_values = QNetwork(state_qn)\n","        action = get_action(q_values, epsilon)\n","\n","        # Take action A and receive reward R and the next state S'\n","        next_state, reward, done, _ = env.step(action)\n","\n","        # Store experience tuple (S,A,R,S') in the memory buffer\n","        # Storing the done variable for our convenience\n","        memory_buffer.append(experience(state, action, reward, next_state, done))\n","\n","        # Update the network only after every NUM_STEPS_FOR_UPDATE timesteps.\n","        update = check_update_conditions(t, NUM_STEPS_FOR_EPISODE, memory_buffer)\n","\n","        if update :\n","\n","            # Sample random mini-batch of experience tuples (S,A,R,S') from D\n","            experiences = get_experiences(memory_buffer)\n","\n","            # Set the y-targets, perform a gradient descent step,\n","            # and update the network weights\"\n","            agent_learn(experiences, GAMMA)\n","\n","        state = next_state.copy()\n","        totalPoints += reward\n","\n","        if done :\n","            break\n","\n","    totalPointHistory.append(totalPoints)\n","    averageLatestPoints = np.mean(totalPointHistory[-numberPointAverage:])\n","\n","    # Update the Epsilon value\n","    epsilon = get_new_epsilon(epsilon)\n","\n","    print(f\"\\rEpisode {i+1} | Total point average of the last {numberPointAverage} episodes: {averageLatestPoints:.2f}\", end=\"\")\n","\n","    if (i + 1) % numberPointAverage == 0 :\n","        print(f\"\\rEpisode {i+1} | Total point average of the last {numberPointAverage} episodes: {averageLatestPoints:.2f}\")\n","\n","totalTimeTaken = time.time() - start\n","\n","print(f\"Total Runtime of the Agent Training: {totalTimeTaken:.2f} s ({(totalTimeTaken / 60):.2f} mins)\")\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNzfvE6hzQ81t8bkazFT2d7","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":0}
