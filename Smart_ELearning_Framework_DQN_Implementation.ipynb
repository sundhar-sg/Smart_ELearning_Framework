{"cells":[{"cell_type":"markdown","metadata":{"id":"Vi1nhs_6_NnV"},"source":["## Smart E - Learning Framework Using Deep Q - Network of Reinforcement Learning"]},{"cell_type":"markdown","metadata":{"id":"kQn4bzRR_y5S"},"source":["### Installing the Necessary Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":109420,"status":"ok","timestamp":1709106373938,"user":{"displayName":"Sundhar Gopal","userId":"16938864680908115120"},"user_tz":-330},"id":"wFImd1ao_2jn","outputId":"0b2c18b4-9568-432a-9904-db54d7283f93"},"outputs":[],"source":["!pip install gymnasium\n","!pip install \"gymnasium[atari, accept-rom-license]\"\n","!apt-get install swig\n","!pip install gymnasium[box2d]"]},{"cell_type":"markdown","metadata":{"id":"CUDsmBeX_ct9"},"source":["### Importing the Libraries"]},{"cell_type":"code","execution_count":50,"metadata":{"executionInfo":{"elapsed":5006,"status":"ok","timestamp":1709106378937,"user":{"displayName":"Sundhar Gopal","userId":"16938864680908115120"},"user_tz":-330},"id":"tPTc-3H__e25"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","\n","import tensorflow as tf\n","import gymnasium as gym\n","from gymnasium import spaces\n","\n","from collections import deque, namedtuple\n","\n","from tensorflow import keras\n","from keras import Sequential\n","from keras.layers import Dense, Input\n","from keras.losses import MSE\n","from keras.optimizers import Adam"]},{"cell_type":"markdown","metadata":{"id":"7H7e-uVm2Fpd"},"source":["### Defining the States for the Environment"]},{"cell_type":"code","execution_count":51,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1709106378937,"user":{"displayName":"Sundhar Gopal","userId":"16938864680908115120"},"user_tz":-330},"id":"_brb4IZl2JGa","outputId":"cc90de95-fced-47ad-eafc-5eefc00860fa"},"outputs":[{"name":"stdout","output_type":"stream","text":["State 1: Start\n","State 2: Reading\n","State 3: Watching\n","State 4: Entertaining\n","State 5: Getting Bored\n","State 6: Rest\n","State 7: Writing\n","State 8: Playing\n","State 9: Clicking Ad\n","State 10: Course Completion\n","State 11: Quit\n"]}],"source":["states = ['Start', 'Reading', 'Watching', 'Entertaining', 'Getting Bored', 'Rest', 'Writing', 'Playing', 'Clicking Ad', 'Course Completion', 'Quit']\n","\n","for i in range(len(states)) :\n","    print(\"State {}: {}\".format(i+1, states[i]))"]},{"cell_type":"markdown","metadata":{"id":"gnhNxfoe2y5G"},"source":["### Defining the Actions for the Environment"]},{"cell_type":"code","execution_count":52,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1709106378938,"user":{"displayName":"Sundhar Gopal","userId":"16938864680908115120"},"user_tz":-330},"id":"TcjC9HL023MA","outputId":"2ce1da02-00bb-41ab-99e4-926cf0975b25"},"outputs":[{"name":"stdout","output_type":"stream","text":["Action 1: Stay\n","Action 2: Previous Course\n","Action 3: Additional Content\n","Action 4: Do assignment\n","Action 5: Move to Exams\n","Action 6: Move to high-level course\n","Action 7: Move to low-level course\n","Action 8: Move to Social Media\n"]}],"source":["actions = ['Stay', 'Previous Course', 'Additional Content', 'Do assignment', 'Move to Exams', 'Move to high-level course', 'Move to low-level course', 'Move to Social Media']\n","\n","for i in range(len(actions)) :\n","    print(\"Action {}: {}\".format(i+1, actions[i]))"]},{"cell_type":"markdown","metadata":{"id":"OyzJpoWK4oH8"},"source":["### Assigning Rewards for each action to be taken respectively"]},{"cell_type":"code","execution_count":53,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1709106378938,"user":{"displayName":"Sundhar Gopal","userId":"16938864680908115120"},"user_tz":-330},"id":"t_wntOap4yT7","outputId":"cd365ea9-6482-4f08-c74d-11d86cbc3555"},"outputs":[{"name":"stdout","output_type":"stream","text":["Action: Stay\tReward Points: 10\n","Action: Previous Course\tReward Points: 20\n","Action: Additional Content\tReward Points: 50\n","Action: Do assignment\tReward Points: 60\n","Action: Move to Exams\tReward Points: 100\n","Action: Move to high-level course\tReward Points: 100\n","Action: Move to low-level course\tReward Points: 70\n","Action: Move to Social Media\tReward Points: -10\n"]}],"source":["rewards = {}\n","rewardPoints = [10, 20, 50, 60, 100, 100, 70, -10]\n","\n","for i in range(len(actions)) :\n","    rewards['{}'.format(actions[i])] = rewardPoints[i]\n","\n","keys = list(rewards.keys())\n","for i in range(len(rewards)) :\n","    print(\"Action: {}\\tReward Points: {}\".format(keys[i], rewards.get(keys[i])))"]},{"cell_type":"markdown","metadata":{},"source":["### Customizing the State values for the Custom Environment"]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["State: Start\tAssigned State Number: 0.0\n","State: Reading\tAssigned State Number: 1.0\n","State: Watching\tAssigned State Number: 2.0\n","State: Entertaining\tAssigned State Number: 3.0\n","State: Getting Bored\tAssigned State Number: 4.0\n","State: Rest\tAssigned State Number: 5.0\n","State: Writing\tAssigned State Number: 6.0\n","State: Playing\tAssigned State Number: 7.0\n","State: Clicking Ad\tAssigned State Number: 8.0\n","State: Course Completion\tAssigned State Number: 9.0\n","State: Quit\tAssigned State Number: 10.0\n"]}],"source":["customStates = {}\n","\n","currentStateNumber = 0.0\n","\n","for state in states :\n","    customStates['{}'.format(state)] = currentStateNumber\n","    currentStateNumber += 1\n","\n","for key in list(customStates.keys()) :\n","    print(\"State: {}\\tAssigned State Number: {}\".format(key, customStates[key]))\n","\n","finalStateForm = np.array(list(customStates.values())).astype(np.float32)"]},{"cell_type":"markdown","metadata":{"id":"pAHepFAKBFYN"},"source":["### Creating and Registering the Environment in Gymnasium"]},{"cell_type":"code","execution_count":82,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1709106378939,"user":{"displayName":"Sundhar Gopal","userId":"16938864680908115120"},"user_tz":-330},"id":"7TY2KDw_BLAy"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/sundhar-sg/anaconda3/envs/tensorflow/lib/python3.11/site-packages/gymnasium/envs/registration.py:693: UserWarning: \u001b[33mWARN: Overriding environment ELearningEnv-v1 already in registry.\u001b[0m\n","  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n"]}],"source":["class ELearningEnv(gym.Env) :\n","    def __init__(self) :\n","        super(ELearningEnv, self).__init__()\n","        self.states = states\n","        self.actions = actions\n","        self.rewards = rewards\n","        self.action_space = spaces.Discrete(len(self.actions))\n","        self.observation_space = spaces.Discrete(len(self.states))\n","        self.current_state = None\n","\n","    def reset(self) :\n","        self.current_state = 0\n","        return self.current_state\n","\n","    def step(self, action) :\n","        reward = self.rewards.get(self.actions[action], 0)\n","        done = (self.states[self.current_state] in ['Course Completion', 'Quit'])\n","        if not done :\n","            if action == 0 :\n","                pass\n","            elif action == 1 :\n","                self.current_state = max(0, self.current_state - 1)\n","            elif action == 2 :\n","                self.current_state = min(len(self.states) - 1, self.current_state + 1)\n","            elif action == 3 :\n","                self.current_state = min(len(self.states) - 2, self.current_state + 2)\n","            elif action == 4 :\n","                self.current_state = min(len(self.states) - 3, self.current_state + 3)\n","            elif action == 5 :\n","                self.current_state = min(len(self.states) - 4, self.current_state + 4)\n","            elif action == 6 :\n","                self.current_state = min(len(self.states) - 5, self.current_state + 5)\n","            elif action == 7 :\n","                self.current_state = min(len(self.states) - 6, self.current_state + 6)\n","        return self.current_state, reward, done, {}\n","\n","    def render(self, mode = \"human\") :\n","        print(f\"Current State: {self.states[self.current_state]}\")\n","\n","gym.register(id=\"ELearningEnv-v1\", entry_point=\"__main__:ELearningEnv\")"]},{"cell_type":"markdown","metadata":{"id":"Cb8JzWgL86JJ"},"source":["### Setting the Seeding value for the Environment"]},{"cell_type":"code","execution_count":57,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1709106378939,"user":{"displayName":"Sundhar Gopal","userId":"16938864680908115120"},"user_tz":-330},"id":"l0xJGZvo89_Y"},"outputs":[],"source":["tf.random.set_seed(0)"]},{"cell_type":"markdown","metadata":{"id":"iEesj09B9Oa_"},"source":["### Defining the Hyperparameters"]},{"cell_type":"code","execution_count":58,"metadata":{"executionInfo":{"elapsed":404,"status":"ok","timestamp":1709106379335,"user":{"displayName":"Sundhar Gopal","userId":"16938864680908115120"},"user_tz":-330},"id":"_y_mF1rj9RQw"},"outputs":[],"source":["MEMORY_SIZE = 100_000\n","GAMMA = 0.995\n","ALPHA = 1e-3\n","NUM_STEPS_FOR_EPISODE = 4"]},{"cell_type":"markdown","metadata":{"id":"rmnSlEEf9i4x"},"source":["### Defining the Environment for use"]},{"cell_type":"code","execution_count":83,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1709106379336,"user":{"displayName":"Sundhar Gopal","userId":"16938864680908115120"},"user_tz":-330},"id":"HcvOsiZ49u5v","outputId":"1460c902-55ef-42d1-ebbf-3433c15335ef"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of States Available:  11 \n","Number of Actions:  8\n"]},{"name":"stderr","output_type":"stream","text":["/home/sundhar-sg/anaconda3/envs/tensorflow/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:197: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n","  logger.deprecation(\n","/home/sundhar-sg/anaconda3/envs/tensorflow/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:210: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n","  logger.deprecation(\n","/home/sundhar-sg/anaconda3/envs/tensorflow/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:218: UserWarning: \u001b[33mWARN: The result returned by `env.reset()` was not a tuple of the form `(obs, info)`, where `obs` is a observation and `info` is a dictionary containing additional information. Actual type: `<class 'int'>`\u001b[0m\n","  logger.warn(\n"]}],"source":["env = gym.make(\"ELearningEnv-v1\")\n","env.reset()\n","\n","state_size = env.observation_space.n\n","action_size = env.action_space.n\n","\n","print(\"Number of States Available: \", state_size, \"\\nNumber of Actions: \", action_size)"]},{"cell_type":"markdown","metadata":{"id":"q1MFnkNaQaZh"},"source":["### Reset the Environment"]},{"cell_type":"code","execution_count":84,"metadata":{"executionInfo":{"elapsed":19,"status":"ok","timestamp":1709106379336,"user":{"displayName":"Sundhar Gopal","userId":"16938864680908115120"},"user_tz":-330},"id":"hlFTIL8OQfGF"},"outputs":[],"source":["current_state = env.reset()"]},{"cell_type":"markdown","metadata":{"id":"jZUI5csGQu71"},"source":["### Test the Environment for a action"]},{"cell_type":"code","execution_count":85,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1709106379336,"user":{"displayName":"Sundhar Gopal","userId":"16938864680908115120"},"user_tz":-330},"id":"FADOf4DJQybA","outputId":"43db24f0-77f9-4fe3-c61f-b1b0ca96c67a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Current State: Entertaining\n"]},{"name":"stderr","output_type":"stream","text":["/home/sundhar-sg/anaconda3/envs/tensorflow/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:242: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n","  logger.deprecation(\n"]}],"source":["action = 4\n","\n","next_state, reward, done, _ = env.step(action)\n","\n","env.render()"]},{"cell_type":"markdown","metadata":{"id":"q_Vh7aXWU9_R"},"source":["### Create the Deep Neural Network for the Q - Network"]},{"cell_type":"code","execution_count":70,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1709106379337,"user":{"displayName":"Sundhar Gopal","userId":"16938864680908115120"},"user_tz":-330},"id":"pYydVBrZVDku"},"outputs":[],"source":["QNetwork = Sequential(\n","    [\n","        Input(shape=state_size),\n","        Dense(units=64, activation=\"relu\"),\n","        Dense(units=128, activation=\"relu\"),\n","        Dense(units=64, activation=\"relu\"),\n","        Dense(units=action_size, activation=\"linear\")\n","    ]\n",")\n","\n","target_QNetwork = Sequential(\n","    [\n","        Input(shape=state_size),\n","        Dense(units=64, activation=\"relu\"),\n","        Dense(units=128, activation=\"relu\"),\n","        Dense(units=64, activation=\"relu\"),\n","        Dense(units=action_size, activation=\"linear\")\n","    ]\n",")\n","\n","optimizer = Adam(learning_rate=ALPHA)"]},{"cell_type":"markdown","metadata":{"id":"lczeVADlfJjy"},"source":["### Implementing Experience Replay"]},{"cell_type":"code","execution_count":71,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1709106379337,"user":{"displayName":"Sundhar Gopal","userId":"16938864680908115120"},"user_tz":-330},"id":"XURocGktfQ-o"},"outputs":[],"source":["experience = namedtuple(typename=\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])"]},{"cell_type":"markdown","metadata":{"id":"yCGQaH5lgQk0"},"source":["### Defining Compute Loss function for the Deep Q - Learning with Experience Replay"]},{"cell_type":"code","execution_count":72,"metadata":{"executionInfo":{"elapsed":401,"status":"ok","timestamp":1709108553326,"user":{"displayName":"Sundhar Gopal","userId":"16938864680908115120"},"user_tz":-330},"id":"OnAeyxWwgXIx"},"outputs":[],"source":["def compute_loss(experiences, gamma, QNetwork, target_QNetwork) :\n","\n","    # Unpack the mini-batch of experiences\n","    states, actions, rewards, next_states, done_vals = experiences\n","\n","    # Compute max Q^(s,a)\n","    max_qsa = tf.reduce_max(input_tensor=target_QNetwork(next_states), axis=-1)\n","\n","    # Set y = R if episode terminates, otherwise set y = R + gamma max Q^(s,a)\n","    yTargets = rewards + (gamma * max_qsa * (1 - done_vals))\n","\n","    # Get the q_values and reshape to match y_targets\n","    q_values = QNetwork(states)\n","    q_values = tf.gather_nd(q_values, tf.stack([tf.range(q_values.shape[0]),\n","                                                tf.cast(actions, tf.int32)], axis = 1))\n","    \n","    # Compute the Loss\n","    loss = MSE(yTargets, q_values)\n","\n","    return loss"]},{"cell_type":"markdown","metadata":{},"source":["### Update the Target Network Weights"]},{"cell_type":"code","execution_count":73,"metadata":{},"outputs":[],"source":["TAU = 1e-3 # Soft update parameter\n","\n","def update_target_network(QNetwork, target_QNetwork) :\n","    for targetWeights, QNetWeights in zip(target_QNetwork.weights, QNetwork.weights) :\n","        targetWeights.assign(TAU * QNetWeights + (1.0 - TAU) * targetWeights)"]},{"cell_type":"markdown","metadata":{},"source":["### Getting the Action based on Epsilon-greedy policy"]},{"cell_type":"code","execution_count":74,"metadata":{},"outputs":[],"source":["import random\n","\n","def get_action(q_values, epsilon=0.0) :\n","\n","    # Returns an action based on the Epsilon-greedy policy\n","\n","    if random.random() > epsilon :\n","        return np.argmax(q_values.numpy()[0])\n","    return random.choice(np.arange(8))"]},{"cell_type":"markdown","metadata":{},"source":["### Checking the Update conditions for the network"]},{"cell_type":"code","execution_count":75,"metadata":{},"outputs":[],"source":["MINIBATCH_SIZE = 64\n","\n","def check_update_conditions(t, num_steps_upd, memory_buffer) :\n","\n","    # Returns a boolean that will be True if the conditions are met and False otherwise.\n","\n","    if (t + 1) % num_steps_upd == 0 and len(memory_buffer) > MINIBATCH_SIZE :\n","        return True\n","    return False"]},{"cell_type":"markdown","metadata":{},"source":["### Getting the Experiences Randomly from the Memory Buffer"]},{"cell_type":"code","execution_count":76,"metadata":{},"outputs":[],"source":["def get_experiences(memory_buffer) :\n","\n","    # Returns a random sample of experience tuples drawn from the memory buffer.\n","    experiences = random.sample(memory_buffer, k=MINIBATCH_SIZE)\n","    states = tf.convert_to_tensor(\n","        np.array([e.state for e in experiences if e is not None]), dtype=tf.float32\n","    )\n","    actions = tf.convert_to_tensor(\n","        np.array([e.action for e in experiences if e is not None]), dtype=tf.float32\n","    )\n","    rewards = tf.convert_to_tensor(\n","        np.array([e.reward for e in experiences if e is not None]), dtype=tf.float32\n","    )\n","    next_states = tf.convert_to_tensor(\n","        np.array([e.next_state for e in experiences if e is not None]), dtype=tf.float32\n","    )\n","    done_vals = tf.convert_to_tensor(\n","        np.array([e.done for e in experiences if e is not None]).astype(np.uint8),\n","        dtype=tf.float32,\n","    )\n","    return (states, actions, rewards, next_states, done_vals)"]},{"cell_type":"markdown","metadata":{},"source":["### Updating the Epsilon value"]},{"cell_type":"code","execution_count":77,"metadata":{},"outputs":[],"source":["EPSILON_MIN = 0.01 # Minimum Epsilon value for the Epsilon-greedy policy\n","EPSILON_DECAY = 0.995 # Epsilon-decay rate for the Epsilon-greedy policy\n","\n","def get_new_epsilon(epsilon) :\n","\n","    # Returns the updated Epsilon value for the Epsilon-greedy policy\n","\n","    return max(EPSILON_MIN, EPSILON_DECAY * epsilon)"]},{"cell_type":"markdown","metadata":{},"source":["### Update the Network Weights"]},{"cell_type":"code","execution_count":78,"metadata":{},"outputs":[],"source":["@tf.function\n","def agent_learn(experiences, gamma) :\n","    \n","    # Calculate the loss\n","    with tf.GradientTape() as tape :\n","        loss = compute_loss(experiences, gamma, QNetwork, target_QNetwork)\n","\n","    # Get the Gradients of the loss with respect to the weights\n","    gradients = tape.gradient(loss, QNetwork.trainable_variables)\n","\n","    # Update the weights of the QNetwork\n","    optimizer.apply_gradients(zip(gradients, QNetwork.trainable_variables))\n","\n","    # Update the Weights of the Target QNetwork\n","    update_target_network(QNetwork, target_QNetwork)"]},{"cell_type":"markdown","metadata":{},"source":["### Training the Agent"]},{"cell_type":"code","execution_count":81,"metadata":{},"outputs":[{"ename":"ValueError","evalue":"Exception encountered when calling layer 'sequential_2' (type Sequential).\n\nInput 0 of layer \"dense_8\" is incompatible with the layer: expected min_ndim=2, found ndim=1. Full shape received: (1,)\n\nCall arguments received by layer 'sequential_2' (type Sequential):\n  • inputs=tf.Tensor(shape=(1,), dtype=float32)\n  • training=None\n  • mask=None","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[81], line 29\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(maximumNumberTimesteps) :\n\u001b[1;32m     26\u001b[0m \n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# From the current state S, choose an action A using an Epsilon-greedy policy\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     state_qn \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(state, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;66;03m# State needs to be the right shape for the Q-Network\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m     q_values \u001b[38;5;241m=\u001b[39m \u001b[43mQNetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_qn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     action \u001b[38;5;241m=\u001b[39m get_action(q_values, epsilon)\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# Take action A and receive reward R and the next state S'\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.11/site-packages/keras/src/engine/input_spec.py:253\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    251\u001b[0m     ndim \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mrank\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ndim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m ndim \u001b[38;5;241m<\u001b[39m spec\u001b[38;5;241m.\u001b[39mmin_ndim:\n\u001b[0;32m--> 253\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    254\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    255\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis incompatible with the layer: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    256\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected min_ndim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;241m.\u001b[39mmin_ndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    257\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound ndim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    258\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFull shape received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(shape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    259\u001b[0m         )\n\u001b[1;32m    260\u001b[0m \u001b[38;5;66;03m# Check dtype.\u001b[39;00m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer 'sequential_2' (type Sequential).\n\nInput 0 of layer \"dense_8\" is incompatible with the layer: expected min_ndim=2, found ndim=1. Full shape received: (1,)\n\nCall arguments received by layer 'sequential_2' (type Sequential):\n  • inputs=tf.Tensor(shape=(1,), dtype=float32)\n  • training=None\n  • mask=None"]}],"source":["import time\n","\n","start = time.time()\n","\n","numberOfEpisodes = 100\n","maximumNumberTimesteps = 500\n","\n","totalPointHistory = []\n","\n","numberPointAverage = 10 # Number of total points to use for averaging\n","epsilon = 1.0 # Initial Epsilon value for Epsilon-greedy policy\n","\n","# Create a memory buffer D with capacity N\n","memory_buffer = deque(maxlen=MEMORY_SIZE)\n","\n","# Set the target network weights equal to the Q-Network weights\n","target_QNetwork.set_weights(QNetwork.get_weights())\n","\n","for i in range(numberOfEpisodes) :\n","\n","    # Reset the environment to the initial state and get the initial state\n","    state = env.reset()\n","    totalPoints = 0\n","\n","    for t in range(maximumNumberTimesteps) :\n","\n","        # From the current state S, choose an action A using an Epsilon-greedy policy\n","        state_qn = np.expand_dims(state, axis=0) # State needs to be the right shape for the Q-Network\n","        q_values = QNetwork(state_qn)\n","        action = get_action(q_values, epsilon)\n","\n","        # Take action A and receive reward R and the next state S'\n","        next_state, reward, done, _ = env.step(action)\n","\n","        # Store experience tuple (S,A,R,S') in the memory buffer\n","        # Storing the done variable for our convenience\n","        memory_buffer.append(experience(state, action, reward, next_state, done))\n","\n","        # Update the network only after every NUM_STEPS_FOR_UPDATE timesteps.\n","        update = check_update_conditions(t, NUM_STEPS_FOR_EPISODE, memory_buffer)\n","\n","        if update :\n","\n","            # Sample random mini-batch of experience tuples (S,A,R,S') from D\n","            experiences = get_experiences(memory_buffer)\n","\n","            # Set the y-targets, perform a gradient descent step,\n","            # and update the network weights\"\n","            agent_learn(experiences, GAMMA)\n","\n","        state = next_state.copy()\n","        totalPoints += reward\n","\n","        if done :\n","            break\n","\n","    totalPointHistory.append(totalPoints)\n","    averageLatestPoints = np.mean(totalPointHistory[-numberPointAverage:])\n","\n","    # Update the Epsilon value\n","    epsilon = get_new_epsilon(epsilon)\n","\n","    print(f\"\\rEpisode {i+1} | Total point average of the last {numberPointAverage} episodes: {averageLatestPoints:.2f}\", end=\"\")\n","\n","    if (i + 1) % numberPointAverage == 0 :\n","        print(f\"\\rEpisode {i+1} | Total point average of the last {numberPointAverage} episodes: {averageLatestPoints:.2f}\")\n","\n","totalTimeTaken = time.time() - start\n","\n","print(f\"Total Runtime of the Agent Training: {totalTimeTaken:.2f} s ({(totalTimeTaken / 60):.2f} mins)\")\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNzfvE6hzQ81t8bkazFT2d7","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":0}
